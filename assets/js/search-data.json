{
  
    
        "post0": {
            "title": "Understanding Kepler Data",
            "content": "Technical Preliminaries . # Enable immediate usage of Python script files %reload_ext autoreload %autoreload 2 # Plot figures inline %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . import sys #sys.path.append(&#39;/Users/itacdonev/Documents/PROJECTS/AstroAI&#39;) #sys.path.append(&#39;/Users/itacdonev/Documents/PROJECTS/AstroAI/code&#39;) sys.path . [&#39;/Users/itacdonev/Documents/PROJECTS/AstroAI/astroai/notebooks&#39;, &#39;/Users/itacdonev/anaconda3/envs/astro/lib/python37.zip&#39;, &#39;/Users/itacdonev/anaconda3/envs/astro/lib/python3.7&#39;, &#39;/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/lib-dynload&#39;, &#39;&#39;, &#39;/Users/itacdonev/.local/lib/python3.7/site-packages&#39;, &#39;/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/site-packages&#39;, &#39;/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/site-packages/IPython/extensions&#39;, &#39;/Users/itacdonev/.ipython&#39;] . sys.path.append(&#39;/Users/itacdonev/Documents/PROJECTS/AstroAI/astroai&#39;) from external.light_curve import binning . # IMORT PACKAGES # import gc import pandas as pd import numpy as np import random # Visualization import seaborn as sns import matplotlib.pyplot as plt from termcolor import colored, cprint from astropy.io import fits # Import fits files from astropy.table import Table # Converting to tidy data tables import tensorflow as tf tf.enable_eager_execution() # # Source: Ita Cirovic Donev import utils # Source: Chris Shaulle from external.light_curve import kepler_io . # Define the main data path DATA_PATH = &#39;../data/&#39; . INTRODUCTION . In this notebook we will go over the Kepler mission data, which we will later structure and use to train ML and DL models. Goals of this notebook: . what to download | how to download | undertsanding what we have in data | plotting data | . What to Download? . Given that we want to build a model using the supervised learning algorithm we need a set of labeled data. To detect the potential planet we will be using the transit method. From previous notebook we understood what the transit method is. Hence, our training data is based on the light curves of a star and its labeled class. . Therefore, we need to acquire two sets of data: labeled data and the light curves (time series). . The labeled TCE data, namely the DR24 TCE table we will obtain from the (NASA Exoplanet Archive) | Light curves of the stars corresponding to the DR24 table we can obtained from the (Mikulski Archive) | All the data is saved in the folder ../data/. . DR24 TCE data . The complete description of available data columns can be found in Data Columns in the Kepler TCE Table. From all the available columns, we will download the following: . kepid - Target identification number, as listed in the Kepler Input Catalog (KIC). The KIC was derived from a ground-based imaging survey of the Kepler field conducted prior to launch. The survey&#39;s purpose was to identify stars for the Kepler exoplanet survey by magnitude and color. The full catalog of 13 million sources can be searched at the MAST archive. The subset of 4 million targets found upon the Kepler CCDs can be searched via the Kepler Target Search form. The Kepler ID is unique to a target and there is only one Kepler ID per target. | tce_plnt_num - Planet number | tce_period - Orbital period (days): The interval between consecutive planetary transits. | tce_time0bk - Transit Epoch (BJD - 2,454,833.0): The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC. | tce_duration - Transit Duration (hrs): The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris. | av_training_set - Autovetter Training Set Label: If the TCE was included in the training set, the training label encodes what is believed to be the &quot;true&quot; classification, and takes a value of either PC, AFP or NTP. The TCEs in the UNKNOWN class sample are marked UNK. Training labels are given a value of NULL for TCEs not included in the training set. For more detail about how the training set is constructed, see Autovetter Planet Candidate Catalog for Q1-Q17 Data Release 24 (KSCI-19091). PC: planet candidate | AFP: astrophysical false positive | NTP: non-transiting phenomenon | UNK: unknown | . | av_pred_class - Autovetter Predicted Classification: Predicted classifications, which are the &quot;optimum MAP classifications.&quot; Values are either PC, AFP, or NTP. | . There are two options to download the data: manually or using the API. For manual download go to NASA Exoplanet Archive and select the appropriate columns. If you would like to use the API just run the following in your terminal: . python -m data.get_tce_data --output_file=dr24_tce_labels.csv ./get_tce.sh . where for the argument output_file enter the desired file name. The default location for the saved file is ../data/. Note that this is run from the main root of the project. . Light curves for DR24 TCE data . The Mikulski archive contains all the Kepler mission data, which as you can image is huge. Since we only have labels for a subset of the dataset, we will just download that specific subset, i.e. the DR24. The process of downloading the is closely followed with the code by Shaulle. We will use the light_curve package and its corresponding py scripts. The light curves are downloaded using the generate_download_script.py from the light_curve package by Schaulle using the get_data.shby running the following commands: . sh get_data.sh ./get_kepler.sh . The data of 90GB is downloaded in the ../data/kepler/ folder. Note that the download takes several days. . Examining the Data . Let&#39;s see what we have downloaded. . # List all files and folder in DATA_PATH utils.get_files(DATA_PATH) . Target Labels . # Read the data for the target labels df_y = pd.read_csv(f&#39;{DATA_PATH}dr24_tce_labels.csv&#39;) print(colored(f&#39;Number of IDs: {df_y.shape[0]}&#39;,&#39;blue&#39;)) df_y.head() . The column av_training_set provides the target labels. Let&#39;s see how many labels we have: . df_y[&#39;av_training_set&#39;].value_counts() . There are 3600 planet candidates in the training set. We can create a binary target with 1 only for PC target label and 0 for the remaining. . # Create binary target df_y[&#39;target&#39;] = np.where(df_y[&#39;av_training_set&#39;] == &#39;PC&#39;, 1, 0) . In every classification problem we need to check the level of imbalance of the dataset, i.e. the proportion of 1s to the whole dataset. In our example this is equal to: . utils.check_target(df_y, &#39;target&#39;) . Light Curves . Now, let&#39;s see the Kepler data on light curves. . # Enter which Kepler ID to plot KEPLER_ID = 11442793 #Kepler-90 KEPLER_DATA_DIR = f&#39;{DATA_PATH}kepler/&#39; LABEL = df_y[df_y.kepid == KEPLER_ID][&#39;av_training_set&#39;].iloc[0,] # Extract target label # Default ploting style plt.style.use(&#39;ggplot&#39;) . # Get file names for the KEPLER_ID file_names = kepler_io.kepler_filenames(KEPLER_DATA_DIR, KEPLER_ID) assert file_names, f&#39;Failed to find .fits files in {KEPLER_DATA_DIR}&#39; file_names . The file names have the following format (taken from the light_curve.kepler_io.py): . ${kep_id:0:4}/${kep_id}/kplr${kep_id}-${quarter_prefix}_${type}.fits . where: . kep_id is the Kepler id left-padded with zeros to length 9; | quarter_prefix is the filename quarter prefix; | type is one of &quot;llc&quot; (long cadence light curve) or &quot;slc&quot; (short cadence light curve). | . Before plotting the light curves let&#39;s examine the *.fits files. To read these files we will need the astropy package. Taking just one *.fits file from file_names and extracting the information contained in the file. . # Take the first fits file from file_names f = file_names[0] fits.info(f) . As we can see there are 3 main HUDs or header data units: . No. 0 (Primary): This HDU (Header Data Units) contains meta-data related to the entire file. | No. 1 (Light curve): This HDU contains a binary table that holds data like flux measurements and times. We will extract information from here when we define the parameters for the light curve plot. | No. 2 (Aperture): This HDU contains the image extension with data collected from the aperture. We will also use this to display a bitmask plot that visually represents the optimal aperture used to create the SAP_FLUX column in HDU1. | . For more detailed description of FITS headers please refer to Section 2.1.3. in the Kepler Archive Manual. Reference: MAST Notebook examples . Now, let&#39;s see how to extract information from each specific HDU. . with fits.open(file_names[0]) as hdulist: HDU_LC = hdulist[&#39;LIGHTCURVE&#39;].header print(f&#39;Number of columns: {len(HDU_LC)}&#39;) print(&#39;-&#39;*30) print(repr(HDU_LC[:25])) # Show first 25 column information . We can also see the actual values of each column: . with fits.open(f) as hdulist: LC_table = hdulist[&#39;LIGHTCURVE&#39;].data LC_table = Table(LC_table) LC_table[:5] # Show first 5 rows . Consulting the Kepler Archive Manual (Section 2.3.1.) we need information on the following columns: . TIME = The time at the mid-point of the cadence in BKJD. Convert to BJD using the following formula BJD_i = TIME_i + BJDREFI + BJDREFF where BJDREFI and BJDREFF are keywords in the header. . | SAP_FLUX = The flux in units of electrons per second contained in the optimal aperture pixels collected by the spacecraft. This light curve is the output of the PA module in the SOC pipeline. . | PDCSAP_FLUX = The flux contained in the optimal aperture in electrons per second after the PDC module has applied its cotrending algorithm to the PA light curve. To better understand how PDC manipulated the light curve, read Section 2.3.1.2 and see the PDCSAPFL keyword in the header. . | . Each of the .fits files contains data per specific quarter. We need to combine all from each file to plot the complete time series. So lets define flux and time to be the two variables where we will append timestamps and the corresponding flux or the brigthness of the star. Then going through all the file_names using the astropy package we extract data for time and the flux. Note that we will take the column PDCSAP_FLUX. We will get an array with length equal to the number of .fits files, i.e. 14. . flux = [] time = [] # Extract data for time and flux for f in file_names: with fits.open(f) as hdulist: flux.append(hdulist[&#39;LIGHTCURVE&#39;].data[&#39;PDCSAP_FLUX&#39;]) time.append(hdulist[&#39;LIGHTCURVE&#39;].data[&#39;TIME&#39;]) # Print number of observations for each quarter period for i in range(len(flux)): print(f&#39;N = {len(flux[i])}&#39;) . Let&#39;s see what we have in the flux and time objects: . nan_flux = 0 for i in flux: # Sum the number of non-finite values from all quarter periods nan_flux += (np.isnan(i)).sum() print(f&#39;There are {nan_flux} non-finite values in the timeseries.&#39;) . As we can see there are nan values, which we need to clean up before any further analysis. We will use the numpy method logical_and which computes the truth value of x1 AND x2 element-wise, where in our case x1 is time and x2 is flux. As the result we will obtain an array with values [True, False, ... , True] depending on whether both the time and flux for that particular value in the array is a finite value, with True for both finite and False otherwise. Then we will only select the values of the original time series where we obtained True value from the logical check. Hence, we will shorten the time series, but in essence since there were nan values, we didn&#39;t have them in the first place. So all is good! . # Remove non-finite values from the time series array for i, (t, f) in enumerate(zip(time, flux)): t_f_finite = np.logical_and(np.isfinite(t), np.isfinite(f)) time[i] = t[t_f_finite] flux[i] = f[t_f_finite] # Print number of observations for each quarter period print(f&#39;N = {len(flux[i])}&#39;) . Now that we have the full dataset, i.e. without any non-finite values, let&#39;s plot the time series for the selected Kepler ID. For easier readjustments of the figures, let&#39;s define some plotting arguments upfront, like color and opacity. . color = &#39;#371F72&#39; alpha = 0.8 . plt.figure(figsize=(15,6)) plt.plot(np.concatenate(time), np.concatenate(flux), &#39;.&#39;, c = color, alpha = alpha) plt.xlabel(&#39;Time (days)&#39;) plt.ylabel(&#39;Brigthness&#39;) plt.show() . We can see that the time series for each quarter period is on different scale. (REVIEW) The differences arise due to the repositioning of the Kepler telescope for the purposes of optimizing the solar panels and accumulating solar energy. Since we want a one time series we need to rescale all the quarter periods. We will achieve this by dividing the time series with the median of each quarter period. First let&#39;s try to plot just one quarter period to see the change in brigthness more clearly. . # Divide values in each quarter period by that period&#39;s median value for f in flux: f /= np.median(f) . Now let&#39;s plot the complete time series again. As we can see the time series is centered at 1 and much easier to read.. Notice the two planets in the figure? . plt.figure(figsize=(15,6)) plt.plot(np.concatenate(time), np.concatenate(flux), &#39;.&#39;, c = color, alpha = alpha) plt.xlabel(&#39;Time (days)&#39;) plt.ylabel(&#39;Brigthness&#39;) plt.show() . Going back to the quarter periods, let&#39;s choose one and plot only that time period for more detailed viewing capabilities. To illustrate the structure of light curves better we choose the third quarter period where a clear decrease in brigthness ca be spotted. . q = 3 # Select the quarter period to plot plt.figure(figsize=(15,6)) plt.plot(time[q], flux[q], &#39;.&#39;, c = color, alpha = alpha) plt.xlabel(&#39;Time (days)&#39;) plt.ylabel(&#39;Brigthness&#39;) plt.show() . Ploting light curves (the shorter way) . Let&#39;s plot the light curves for the first 10 Kepler IDs in the dataset using the function view_kepler. . all_time, all_flux = utils.get_light_curves(KEPLER_DATA_DIR, KEPLER_ID) . utils.plot_light_curve(all_time, all_flux, KEPLER_ID, LABEL, color, alpha, quarter = 3) . utils.plot_light_curve(all_time, all_flux, KEPLER_ID, LABEL, color, alpha) . Combining the data extraction and ploting into one function view_kepler we can use the following one line to get the light curves: . utils.view_kepler(KEPLER_DATA_DIR, KEPLER_ID, LABEL, color, alpha) . Now let&#39;s radomly view the Kepler data by chosing, at random, 5 Kepler IDs for which to plot the time series. We will choose the index value of the length of the data set i.e. from the total number of data entries in the TCE labels dataset. Then using the choosen index we extract the ID and the label from kepid and av_training_set columns respectively. . n = 5 # Number of Kepler ID to plot # Randomly choose n number of IDs list = range(len(df_y[&#39;kepid&#39;])) rnd_ids = random.choices(list, k = n) # Plot light curves for each Kepler ID for i in rnd_ids: kep_id = df_y.loc[i][&#39;kepid&#39;] label = df_y.loc[i][&#39;av_training_set&#39;] utils.view_kepler(KEPLER_DATA_DIR, kep_id, label, color, alpha) . Aperture Data . Similarly, we can also explore the APERTURE HDU. The process of reading the data is the same as for the light curves we just select the HDU as follows: . with fits.open(file_names[0]) as hdulist: HDU_APT = hdulist[&#39;APERTURE&#39;].header print(f&#39;Number of columns: {len(HDU_APT)}&#39;) print(&#39;-&#39;*30) print(repr(HDU_APT)) # Show first 25 column information . # Aperture data with fits.open(file_names[3]) as hdulist: apt_data = hdulist[&#39;APERTURE&#39;].data print(apt_data) . nrow = 4 ncol = 4 #fig, ax = plt.subplots(nrows = nrow, ncols = ncol, figsize = (3,3)) fig = plt.figure(figsize = (20,20)) #fig.subplots_adjust(hspace = 0.05) fig.suptitle(f&#39;Kepler Apertures for Kepler ID = {KEPLER_ID}&#39;) for i in range(len(file_names)): with fits.open(file_names[i]) as hdulist: apt_data = hdulist[&#39;APERTURE&#39;].data # Plot each image #plt.subplot(2,7, sharex = &#39;col&#39;, sharey = &#39;row&#39;) ax = fig.add_subplot(nrow, ncol, (i+1)) img = ax.imshow(apt_data, cmap = plt.cm.YlGnBu_r) plt.title(f&#39;Quarter period:&#39;) fig.colorbar(img, ax = ax) plt.show() . TFRecord View - Downloaded preprocessed data . import os . tf_path = os.path.join(DATA_PATH, &#39;tfrecord&#39;) tf_path = [os.path.join(tf_path,f,&#39;.tfrecord&#39;) for f in os.listdir(tf_path)] tf_path . NameError Traceback (most recent call last) &lt;ipython-input-2-bab87f60a61c&gt; in &lt;module&gt; -&gt; 1 tf_path = os.path.join(DATA_PATH, &#39;tfrecord&#39;) 2 tf_path = [os.path.join(tf_path,f,&#39;.tfrecord&#39;) for f in os.listdir(tf_path)] 3 tf_path NameError: name &#39;DATA_PATH&#39; is not defined . raw_data = tf.data.TFRecordDataset(&#39;../data/tfrecord/train-00007-of-00008.tfrecord&#39;) raw_data . KEPLER_ID = 11442793 # Kepler-90 TFRECORD_DIR = &quot;../data/tfrecord/&quot; . def find_tce(kepid, tce_plnt_num, filenames): for filename in filenames: for record in tf.python_io.tf_record_iterator(filename): ex = tf.train.Example.FromString(record) if (ex.features.feature[&quot;kepid&quot;].int64_list.value[0] == kepid and ex.features.feature[&quot;tce_plnt_num&quot;].int64_list.value[0] == tce_plnt_num): print(&quot;Found {}_{} in file {}&quot;.format(kepid, tce_plnt_num, filename)) return ex raise ValueError(&quot;{}_{} not found in files: {}&quot;.format(kepid, tce_plnt_num, filenames)) . filenames = tf.gfile.Glob(os.path.join(TFRECORD_DIR, &quot;*&quot;)) assert filenames, &quot;No files found in {}&quot;.format(TFRECORD_DIR) ex = find_tce(KEPLER_ID, 1, filenames) . ex.features.feature[&#39;kepid&#39;] . global_view = np.array(ex.features.feature[&quot;global_view&quot;].float_list.value) local_view = np.array(ex.features.feature[&quot;local_view&quot;].float_list.value) fig, axes = plt.subplots(1, 2, figsize=(20, 6)) axes[0].plot(global_view, &quot;.&quot;) axes[1].plot(local_view, &quot;.&quot;) plt.show() .",
            "url": "https://itacdonev.github.io/ML-Stories/deep%20learning/2020/04/10/AstroAI-Data.html",
            "relUrl": "/deep%20learning/2020/04/10/AstroAI-Data.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Visual Notes for Deep Learning",
            "content": "These are visual notes of the book Deep Learning by Goodfello, I., Bengio Y., and Counrville A. . | | . CH1 - Introduction (pdf) | Linear Algebra (pdf) | .",
            "url": "https://itacdonev.github.io/ML-Stories/book%20notes/2020/04/08/DL-Notes.html",
            "relUrl": "/book%20notes/2020/04/08/DL-Notes.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "",
            "content": "Score: 4/5 . References .",
            "url": "https://itacdonev.github.io/ML-Stories/2020/02/29/BR-Living-Machines.html",
            "relUrl": "/2020/02/29/BR-Living-Machines.html",
            "date": " • Feb 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Pandas: A tutorial",
            "content": "Introduction . In this notebook we will go through the main concepts of pandas library. First, let&#39;s import the pandas library. We use the common alias pd. . import pandas as pd .",
            "url": "https://itacdonev.github.io/ML-Stories/tutorial/2020/02/20/test.html",
            "relUrl": "/tutorial/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Greek tragedy in the 20th century",
            "content": "Score: 4/5 . Last year I read the Uninhabitable Earth by David Wallace-Wells, which halfway through felt like an ice shower being on a circular loop. By the time I reached the end of the book, I felt so horrible for all the children and the “gift” we are so proudly giving, the wrecked planet. What an inheritance! I bet none of them would love us more for it. Then I picked up this book, Losing Earth: The Decade We Could Have Stopped Climate Change, which judging by the title will provide for yet another icy shower. Long story short, it did not disappoint. . Starting strong with the first sentence, “Nearly everything we understand about global warming was understood in 1979.”, meaning we have been incompetent for the past 40 years! I say incompetent since we had all the information needed to start acting on it. Sadly, some are still applauding themselves. . This science history book reads like a thriller but feels like a Greek tragedy, with the plot, riding the structural wave with twists and turns showcasing the battle between the good and the evil. If only we didn’t know how this scenario would end. . Losing Earth provides a detailed historical account of the many events, House and Senate hearings, reports, conferences, that shaped the climate “debate” starting in 1979 and leading up to the Rio Earth Summit in 1992. We follow the journey of the lead character, an environmental lobbyist, Rafe Promenance, and a NASA scientist Jim Hansen, who each in their way battle with the political circle, oil&amp;gas industry, and the general public. Their primary goal is the acknowledgment of the scientific results on the greenhouse effects and the first official actions of a solution. This battle, at times, does not end well for them, as you could imagine. . What to do? It is discouraging to think that with all the information, technology, and advancements we pride ourselves, we are still mostly blind and unable to move past the crawling stage of human development and start walking into a better future. . To end the review, I will borrow a quote from dr. Tyson’s latest book The Letters from An Astrophysicist on the question “What if you are President” to which he answers: . “One objective reality is that our government doesn’t work, not because we have dysfunctional politicians, but because we have dysfunctional voters. As a scientist and educator, my goal, then, is not to become President and lead a dysfunctional electorate, but to enlighten the electorate so they might choose the right leaders in the first place.” . References . Rich, Nathaniel (2020). Losing Earth: A Climate History. PICADOR. Tyson, N. D. G. (2019). Letters from an astrophysicist. New York: W. W. Norton &amp; Company. .",
            "url": "https://itacdonev.github.io/ML-Stories/book%20review/2020/02/15/BR-Losing-Earth.html",
            "relUrl": "/book%20review/2020/02/15/BR-Losing-Earth.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "",
            "content": "Score: 4/5 . References .",
            "url": "https://itacdonev.github.io/ML-Stories/2020/02/02/BR-Why-we-sleep.html",
            "relUrl": "/2020/02/02/BR-Why-we-sleep.html",
            "date": " • Feb 2, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Science diplomacy, a new field?",
            "content": "Score: 4/5 . A new book by Neil deGrasse Tyson always lures me back into the bookstore, and this one is no exception. Letters from an Astrophysicist provides a structurally different book. Here we are privileged to get a view of multidimensional opinions and questions swirling around in our society, paired together with answers which embrace science diplomacy at its best. And all of this in an almost forgotten practice of correspondence of letter writing. . The correspondence is a presentation of numerous letters, received over the years, divided into four sections: . Ethos (The characteristic spirit of culture, manifested in its beliefs and aspirations), | Cosmos (The Universe seen as a well-ordered whole), | Pathos (A plaintive appeal to emotions that already reside within us), and | Kairos (A propitious moment for decision or action). | . These are then further segmented into topics such as hope, IQ, parenting, science denial, school days, UFOs, etc. All of the questions are unique in their way and given their subject matter. For example, from the segment science denial, we have “Where is the proof?”, in which the word proof relates to evolution and the age of the physical universe. The answer dr. Tyson gave here is very to the point, providing enough scientific (non-controversial) information to move from denial to an actual question, which hopefully further leads to research and widening personal information bucket. “Lego” blocks inching upwards. . My biggest takeaway from the book is the nature, structure, and the appropriateness of responses. There are times we can find ourselves in situations where our audience does not share the same opinion about the topic at hand, which can stir emotions on either side. With the advent of social media, these experiences increase manifold. Dr. Tyson enriches the reader with many diverse angles of human thinking and provides a measured, lucid, and leveled response, proving yet again to be a great teacher, not just of science, but of how to be human. This book will provide you with the numerous ways for communication exchanges to continue and end amicably, regardless of which belief, knowledge, or point of view you or your audience is coming from. . The prologue and epilogue provide yet another, even more personal, take on what it takes to succeed in life. The prologue is like a “memoir” of dr. Tyson’s life path, while the epilogue is a “eulogy” to his father. Both are genuinely fascinating and emotional. To “translate” the words, I would say that I see it as a track race with hurdles in which one doesn’t turn back, having regrets or pointing the blame for experiences, because the next hurdle is coming. Just look straight and keep moving. . The book was fun to read, refreshing, entertaining, while not requiring previous knowledge in astrophysics. There are many excellent and thoughtful quotes one can get from this book, but for me, this one is the strongest because it supersedes the other ones. . “Failure is common to us all. But ambitious people use their failures as lessons to heed, as they push forward toward their goals. Don’t fear change. Don’t fear failure. The only thing to fear is loss of ambition.” . Quotes . “One objective reality is that our government doesn’t work, not because we have dysfunctional voters. As a scientist and educator, my goal, then, is not to become President and lead a dysfunctional electorate, but to enlighten the electorate so they might choose the right leaders in the first place.” . References . Tyson, N. D. G. (2019). Letters from an astrophysicist. New York: W. W. Norton &amp; Company. .",
            "url": "https://itacdonev.github.io/ML-Stories/book%20review/2020/01/16/BR-Letters-from-an-Astrophysicist.html",
            "relUrl": "/book%20review/2020/01/16/BR-Letters-from-an-Astrophysicist.html",
            "date": " • Jan 16, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "A pinch of AI?",
            "content": "Score: 4/5 . The research in AI is growing without the rules or regulations, but with the common goal of achieving the general purpose AI. Do you notice the problem? With many notable achievements in the field, the AI revolution started with many jumping on the high-speed train. It seems as if any project somehow sounds and looks better if we add a pinch of AI to it. But does it? Are we driving blindly? Most likely. Some researchers already ceased their research efforts because the results and methods are being used in a way that is not beneficial to society. If you are one of the passengers on the high-speed train, please take the blindfold off and read this book to be able to act accordingly before your research goes a step too far. . Human Compatible is an honest, eye-opening account of the AI potential and the right way forward, its current shortcomings, and, if we choose the wrong path, our doom. The question is, how do we create an AI system so that we don’t need to consider the control issue, or lack of control for that matter, in every scenario of AI application? How do we overcome the problem of the AI system being willing to be shut down? . Russell takes us back in history to revisit the routes taken in human learning, decision making, and actions taken, as well as how we transferred the learning process onto a machine. As we know, many applications have the AI label from speech recognition, vision systems, translation, etc. and many more will come. But our overall mission is to develop the so-called general-purpose AI, which by Russell is “…a method that is applicable across all problem types and works effectively for large and difficult instances while making very few assumptions.” Sounds good, but as he notes in many instances across the book, many precautions need to be taken along the way. For example, how to align AI and human objectives, how to incorporate our uncertainties and irrational decisions? In short, we need algorithms that would show that the end goal is beneficial to humans. . If you want a fresh perspective on what constitutes AI, how to adjust our current practices, or the standard model for an AI, more in line with our uncertain preferences and objectives; then, this book will help you in this quest. Stuart Russell, one of the most prominent persons in the AI world, embarks on the mission to not only examine how and why the AI could go wrong but most importantly what to consider and how to proceed so that we, the humanity, don’t end up on the flimsy and half-broken tree branch of existence. . In many science disciplines, we battle with the question of how our reality will cease to exist. In astronomy, it is in the way of the Sun engulfing the Earth, in biology, it might be some deadly virus that spreads quickly and efficiently, in environmental science it is the climate change, in computer science, it is the AI. Due to the laws of physics, we can’t much argue with the Sun engulfing the Earth scenario, but with the AI, we can change the dooms outcome since we are the ones creating it and its laws. In Human Compatible, Russell provides an eye-opening perspective on how to mitigate the AI risk and achieve the human compatible status. The secret lies in how we define the objectives of an AI system and its ability or understanding of the need to be switched off at any time that the human deems essential. . I enjoyed the book, which makes for a smooth and light read due to the elegant, flowy, and transparent presentation of ideas and concepts despite the complexity of the topic. The voice is loud and steady, providing for yet another dimension of the seriousness of the issue. For readers in the AI world, the book does not require much specific pre-knowledge, where the general public may need a bit more context, aside from the Appendix provided, to understand the reasons for the whole gloom and dusk scenarios. . Quotes . Humans are intelligent to the extent that our actions can be expected to achieve our objectives. . Machines are beneficial to the extent that their actions can be expected to achieve our objectives. . References . Russell, S. (2019). Human Compatible: AI and the Problem of Control. Allen Lane, Penguin Random House, UK. .",
            "url": "https://itacdonev.github.io/ML-Stories/book%20review/2020/01/11/BR-Human-Compatible.html",
            "relUrl": "/book%20review/2020/01/11/BR-Human-Compatible.html",
            "date": " • Jan 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hello! . I’m Ita. This blog mainly gathers my thoughts on things machine learning. Since I adore reading books I will include reiews of my favourite nonfiction books from various fields as I finish reading them. . Enjoy my blog! .",
          "url": "https://itacdonev.github.io/ML-Stories/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Book Reviews",
          "content": "| | | .",
          "url": "https://itacdonev.github.io/ML-Stories/book_reviews/",
          "relUrl": "/book_reviews/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}