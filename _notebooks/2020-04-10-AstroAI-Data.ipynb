{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"UNDERSTANDING KEPLER DATA\"\n",
    "> \"Project: Applying DL methods in exoplanet search\"\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [deep learning]\n",
    "- author: Ita Ćirović Donev\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable immediate usage of Python script files\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plot figures inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/itacdonev/Documents/PROJECTS/AstroAI/astroai/notebooks',\n",
       " '/Users/itacdonev/anaconda3/envs/astro/lib/python37.zip',\n",
       " '/Users/itacdonev/anaconda3/envs/astro/lib/python3.7',\n",
       " '/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Users/itacdonev/.local/lib/python3.7/site-packages',\n",
       " '/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/site-packages',\n",
       " '/Users/itacdonev/anaconda3/envs/astro/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/itacdonev/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append('/Users/itacdonev/Documents/PROJECTS/AstroAI')\n",
    "#sys.path.append('/Users/itacdonev/Documents/PROJECTS/AstroAI/code')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/itacdonev/Documents/PROJECTS/AstroAI/astroai')\n",
    "from external.light_curve import binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMORT PACKAGES\n",
    "#------------------------------\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "from astropy.io import fits # Import fits files\n",
    "from astropy.table import Table # Converting to tidy data tables\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "#------------------------------\n",
    "# Source: Ita Cirovic Donev\n",
    "import utils\n",
    "\n",
    "# Source: Chris Shaulle\n",
    "from external.light_curve import kepler_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main data path\n",
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "In this notebook we will go over the Kepler mission data, which we will later structure and use to train ML and DL models. Goals of this notebook:\n",
    "- what to download\n",
    "- how to download\n",
    "- undertsanding what we have in data\n",
    "- plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Download?\n",
    "Given that we want to build a model using the supervised learning algorithm we need a set of labeled data. To detect the potential planet we will be using the **transit** method. From previous notebook we understood what the transit method is. Hence, our training data is based on the light curves of a star and its labeled class.\n",
    "\n",
    "Therefore, we need to acquire two sets of data: labeled data and the light curves (time series).\n",
    "1. The labeled TCE data, namely the DR24 TCE table we will obtain from the ([NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce))\n",
    "2. Light curves of the stars corresponding to the DR24 table we can obtained from the ([Mikulski Archive](https://archive.stsci.edu))\n",
    "\n",
    "All the data is saved in the folder `../data/`.\n",
    "\n",
    "### DR24 TCE data\n",
    "The complete description of available data columns can be found in [Data Columns in the Kepler TCE Table](https://exoplanetarchive.ipac.caltech.edu/docs/API_tce_columns.html). From all the available columns, we will download the following:\n",
    "- `kepid` - Target identification number, as listed in the Kepler Input Catalog (KIC). The KIC was derived from a ground-based imaging survey of the Kepler field conducted prior to launch. The survey's purpose was to identify stars for the Kepler exoplanet survey by magnitude and color. The full catalog of 13 million sources can be searched at the MAST archive. The subset of 4 million targets found upon the Kepler CCDs can be searched via the Kepler Target Search form. The Kepler ID is unique to a target and there is only one Kepler ID per target.\n",
    "- `tce_plnt_num` - Planet number\n",
    "- `tce_period` - Orbital period (days): The interval between consecutive planetary transits.\n",
    "- `tce_time0bk` - Transit Epoch (BJD - 2,454,833.0): The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.\n",
    "- `tce_duration` - Transit Duration (hrs): The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.\n",
    "- `av_training_set` - Autovetter Training Set Label: If the TCE was included in the training set, the training label encodes what is believed to be the \"true\" classification, and takes a value of either PC, AFP or NTP. The TCEs in the UNKNOWN class sample are marked UNK. Training labels are given a value of NULL for TCEs not included in the training set. For more detail about how the training set is constructed, see [Autovetter Planet Candidate Catalog](https://exoplanetarchive.ipac.caltech.edu/docs/KSCI-19091-001.pdf) for Q1-Q17 Data Release 24 (KSCI-19091).\n",
    "    - `PC`: planet candidate\n",
    "    - `AFP`: astrophysical false positive\n",
    "    - `NTP`: non-transiting phenomenon\n",
    "    - `UNK`: unknown\n",
    "- `av_pred_class` - Autovetter Predicted Classification: Predicted classifications, which are the \"optimum MAP classifications.\" Values are either PC, AFP, or NTP.\n",
    "\n",
    "There are two options to download the data: manually or using the API. For manual download go to [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce) and select the appropriate columns. If you would like to use the API just run the following in your terminal:\n",
    "\n",
    "```bash\n",
    "python -m data.get_tce_data --output_file=dr24_tce_labels.csv\n",
    "./get_tce.sh\n",
    "```\n",
    "\n",
    "where for the argument `output_file` enter the desired file name. The default location for the saved file is `../data/`. Note that this is run from the main root of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light curves for DR24 TCE data\n",
    "The Mikulski archive contains all the Kepler mission data, which as you can image is huge. Since we only have labels for a subset of the dataset, we will just download that specific subset, i.e. the DR24. The process of downloading the is closely followed with the code by Shaulle. We will use the `light_curve` package and its corresponding py scripts. The light curves are downloaded using the `generate_download_script.py` from the `light_curve` package by Schaulle using the `get_data.sh`by running the following commands:\n",
    "\n",
    "```bash\n",
    "sh get_data.sh\n",
    "./get_kepler.sh\n",
    "```\n",
    "\n",
    "The data of 90GB is downloaded in the `../data/kepler/` folder. Note that the download takes several days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Data\n",
    "Let's see what we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files and folder in DATA_PATH\n",
    "utils.get_files(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data for the target labels\n",
    "df_y = pd.read_csv(f'{DATA_PATH}dr24_tce_labels.csv')\n",
    "print(colored(f'Number of IDs: {df_y.shape[0]}','blue'))\n",
    "df_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `av_training_set` provides the target labels. Let's see how many labels we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y['av_training_set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3600 planet candidates in the training set. We can create a binary target with 1 only for `PC` target label and 0 for the remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target\n",
    "df_y['target'] = np.where(df_y['av_training_set'] == 'PC', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every classification problem we need to check the level of imbalance of the dataset, i.e. the proportion of 1s to the whole dataset. In our example this is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_target(df_y, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the Kepler data on light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter which Kepler ID to plot\n",
    "KEPLER_ID = 11442793 #Kepler-90\n",
    "\n",
    "KEPLER_DATA_DIR = f'{DATA_PATH}kepler/'\n",
    "LABEL = df_y[df_y.kepid == KEPLER_ID]['av_training_set'].iloc[0,] # Extract target label\n",
    "\n",
    "# Default ploting style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file names for the KEPLER_ID\n",
    "file_names = kepler_io.kepler_filenames(KEPLER_DATA_DIR, KEPLER_ID)\n",
    "assert file_names, f'Failed to find .fits files in {KEPLER_DATA_DIR}'\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file names have the following format (taken from the `light_curve.kepler_io.py`):  \n",
    "\n",
    "`${kep_id:0:4}/${kep_id}/kplr${kep_id}-${quarter_prefix}_${type}.fits`  \n",
    "\n",
    "where:\n",
    "- `kep_id` is the Kepler id left-padded with zeros to length 9;\n",
    "- `quarter_prefix` is the filename quarter prefix;\n",
    "- `type` is one of \"llc\" (long cadence light curve) or \"slc\" (short cadence light curve).\n",
    "\n",
    "Before plotting the light curves let's examine the `*.fits` files. To read these files we will need the `astropy` package. Taking just one `*.fits` file from `file_names` and extracting the information contained in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first fits file from file_names\n",
    "f = file_names[0]\n",
    "fits.info(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are 3 main HUDs or header data units:\n",
    "- **No. 0 (Primary)**: This HDU (Header Data Units) contains meta-data related to the entire file.\n",
    "- **No. 1 (Light curve)**: This HDU contains a binary table that holds data like flux measurements and times. We will extract information from here when we define the parameters for the light curve plot.\n",
    "- **No. 2 (Aperture)**: This HDU contains the image extension with data collected from the aperture. We will also use this to display a bitmask plot that visually represents the optimal aperture used to create the SAP_FLUX column in HDU1.\n",
    "\n",
    "For more detailed description of FITS headers please refer to Section 2.1.3. in the [Kepler Archive Manual](https://archive.stsci.edu/kepler/manuals/archive_manual.pdf#page=17).     \n",
    "Reference: [MAST Notebook examples](https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/Kepler/Kepler_Lightcurve/kepler_lightcurve.ipynb)\n",
    "\n",
    "Now, let's see how to extract information from each specific HDU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(file_names[0]) as hdulist:\n",
    "    HDU_LC = hdulist['LIGHTCURVE'].header\n",
    "\n",
    "print(f'Number of columns: {len(HDU_LC)}')\n",
    "print('-'*30)\n",
    "print(repr(HDU_LC[:25])) # Show first 25 column information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the actual values of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(f) as hdulist:\n",
    "    LC_table = hdulist['LIGHTCURVE'].data\n",
    "    \n",
    "LC_table = Table(LC_table)\n",
    "LC_table[:5] # Show first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the [Kepler Archive Manual](https://archive.stsci.edu/kepler/manuals/archive_manual.pdf#page=16) (Section 2.3.1.) we need information on the following columns:\n",
    "\n",
    "- **TIME** = The time at the mid-point of the cadence in BKJD. Convert to BJD using the following formula\n",
    "`BJD_i = TIME_i + BJDREFI + BJDREFF` where BJDREFI and BJDREFF are keywords in the header.\n",
    "\n",
    "- **SAP_FLUX** = The flux in units of electrons per second contained in the optimal aperture pixels collected by the spacecraft. This light curve is the output of the PA module in the SOC pipeline.\n",
    "\n",
    "- **PDCSAP_FLUX** = The flux contained in the optimal aperture in electrons per second after the PDC module has applied its cotrending algorithm to the PA light curve. To better understand how PDC manipulated the light curve, read Section 2.3.1.2 and see the PDCSAPFL keyword in the header.\n",
    "\n",
    "Each of the .fits files contains data per specific quarter. We need to combine all from each file to plot the complete time series. So lets define `flux` and `time` to be the two variables where we will append timestamps and the corresponding flux or the brigthness of the star. Then going through all the `file_names` using the `astropy` package we extract data for time and the flux. Note that we will take the column `PDCSAP_FLUX`. We will get an array with length equal to the number of .fits files, i.e. 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = []\n",
    "time = []\n",
    "\n",
    "# Extract data for time and flux\n",
    "for f in file_names:\n",
    "    with fits.open(f) as hdulist:\n",
    "        flux.append(hdulist['LIGHTCURVE'].data['PDCSAP_FLUX'])\n",
    "        time.append(hdulist['LIGHTCURVE'].data['TIME'])\n",
    "        \n",
    "# Print number of observations for each quarter period\n",
    "for i in range(len(flux)):\n",
    "    print(f'N = {len(flux[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have in the `flux` and `time` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_flux = 0\n",
    "for i in flux:\n",
    "    # Sum the number of non-finite values from all quarter periods\n",
    "    nan_flux += (np.isnan(i)).sum()\n",
    "print(f'There are {nan_flux} non-finite values in the timeseries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are `nan` values, which we need to clean up before any further analysis. We will use the `numpy` method `logical_and` which computes the truth value of x1 AND x2 element-wise, where in our case x1 is `time` and x2 is ` flux`. As the result we will obtain an array with values `[True, False, ... , True]` depending on whether both the `time` and `flux` for that particular value in the array is a finite value, with `True` for both finite and `False` otherwise. Then we will only select the values of the original time series where we obtained `True` value from the logical check. Hence, we will shorten the time series, but in essence since there were `nan` values, we didn't have them in the first place. So all is good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-finite values from the time series array\n",
    "for i, (t, f) in enumerate(zip(time, flux)):\n",
    "    t_f_finite = np.logical_and(np.isfinite(t), np.isfinite(f))\n",
    "    time[i] = t[t_f_finite]\n",
    "    flux[i] = f[t_f_finite]\n",
    "    \n",
    "    # Print number of observations for each quarter period\n",
    "    print(f'N = {len(flux[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full dataset, i.e. without any non-finite values, let's plot the time series for the selected Kepler ID. For easier readjustments of the figures, let's define some plotting arguments upfront, like color and opacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#371F72'\n",
    "alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(np.concatenate(time), np.concatenate(flux), '.', c = color, alpha = alpha)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Brigthness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the time series for each quarter period is on different scale. **(REVIEW) The differences arise due to the repositioning of the Kepler telescope for the purposes of optimizing the solar panels and accumulating solar energy.** Since we want a one time series we need to rescale all the quarter periods. We will achieve this by dividing the time series with the median of each quarter period. First let's try to plot just one quarter period to see the change in brigthness more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide values in each quarter period by that period's median value\n",
    "for f in flux:\n",
    "    f /= np.median(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the complete time series again. As we can see the time series is centered at 1 and much easier to read.. Notice the two planets in the figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(np.concatenate(time), np.concatenate(flux), '.', c = color, alpha = alpha)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Brigthness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the quarter periods, let's choose one and plot only that time period for more detailed viewing capabilities. To illustrate the structure of light curves better we choose the third quarter period where a clear decrease in brigthness ca be spotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 3 # Select the quarter period to plot\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(time[q], flux[q], '.', c = color, alpha = alpha)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Brigthness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting light curves (the shorter way)\n",
    "Let's plot the light curves for the first 10 Kepler IDs in the dataset using the function `view_kepler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time, all_flux = utils.get_light_curves(KEPLER_DATA_DIR, KEPLER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_light_curve(all_time, all_flux, KEPLER_ID, LABEL, color, alpha, quarter = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_light_curve(all_time, all_flux, KEPLER_ID, LABEL, color, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the data extraction and ploting into one function `view_kepler` we can use the following one line to get the light curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.view_kepler(KEPLER_DATA_DIR, KEPLER_ID, LABEL, color, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's radomly view the Kepler data by chosing, at random, 5 Kepler IDs for which to plot the time series. We will choose the index value of the length of the data set i.e. from the total number of data entries in the TCE labels dataset. Then using the choosen index we extract the ID and the label from `kepid` and `av_training_set` columns respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # Number of Kepler ID to plot\n",
    "# Randomly choose n number of IDs\n",
    "list = range(len(df_y['kepid']))\n",
    "rnd_ids = random.choices(list, k = n)\n",
    "\n",
    "# Plot light curves for each Kepler ID\n",
    "for i in rnd_ids:\n",
    "    kep_id = df_y.loc[i]['kepid']\n",
    "    label = df_y.loc[i]['av_training_set']\n",
    "    \n",
    "    utils.view_kepler(KEPLER_DATA_DIR, kep_id, label, color, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also explore the `APERTURE` HDU. The process of reading the data is the same as for the light curves we just select the HDU as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(file_names[0]) as hdulist:\n",
    "    HDU_APT = hdulist['APERTURE'].header\n",
    "\n",
    "print(f'Number of columns: {len(HDU_APT)}')\n",
    "print('-'*30)\n",
    "print(repr(HDU_APT)) # Show first 25 column information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperture data\n",
    "with fits.open(file_names[3]) as hdulist:\n",
    "    apt_data = hdulist['APERTURE'].data\n",
    "print(apt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 4\n",
    "ncol = 4\n",
    "#fig, ax = plt.subplots(nrows = nrow, ncols = ncol, figsize = (3,3))\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "#fig.subplots_adjust(hspace = 0.05)\n",
    "fig.suptitle(f'Kepler Apertures for Kepler ID = {KEPLER_ID}')\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    with fits.open(file_names[i]) as hdulist:\n",
    "        apt_data = hdulist['APERTURE'].data\n",
    "    \n",
    "    # Plot each image\n",
    "    #plt.subplot(2,7, sharex = 'col', sharey = 'row')\n",
    "    ax = fig.add_subplot(nrow, ncol, (i+1))\n",
    "    img = ax.imshow(apt_data, cmap = plt.cm.YlGnBu_r)\n",
    "    plt.title(f'Quarter period:')\n",
    "    fig.colorbar(img, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord View - Downloaded preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bab87f60a61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "tf_path = os.path.join(DATA_PATH, 'tfrecord')\n",
    "tf_path = [os.path.join(tf_path,f,'.tfrecord') for f in os.listdir(tf_path)]\n",
    "tf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = tf.data.TFRecordDataset('../data/tfrecord/train-00007-of-00008.tfrecord')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEPLER_ID = 11442793  # Kepler-90\n",
    "TFRECORD_DIR = \"../data/tfrecord/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tce(kepid, tce_plnt_num, filenames):\n",
    "    for filename in filenames:\n",
    "        for record in tf.python_io.tf_record_iterator(filename):\n",
    "            ex = tf.train.Example.FromString(record)\n",
    "            if (ex.features.feature[\"kepid\"].int64_list.value[0] == kepid and\n",
    "                ex.features.feature[\"tce_plnt_num\"].int64_list.value[0] == tce_plnt_num):\n",
    "                print(\"Found {}_{} in file {}\".format(kepid, tce_plnt_num, filename))\n",
    "                return ex\n",
    "    raise ValueError(\"{}_{} not found in files: {}\".format(kepid, tce_plnt_num, filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.gfile.Glob(os.path.join(TFRECORD_DIR, \"*\"))\n",
    "assert filenames, \"No files found in {}\".format(TFRECORD_DIR)\n",
    "ex = find_tce(KEPLER_ID, 1, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.features.feature['kepid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_view = np.array(ex.features.feature[\"global_view\"].float_list.value)\n",
    "local_view = np.array(ex.features.feature[\"local_view\"].float_list.value)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "axes[0].plot(global_view, \".\")\n",
    "axes[1].plot(local_view, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
